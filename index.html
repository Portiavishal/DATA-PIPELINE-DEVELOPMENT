##CREATE A PIPELINE FOR DATA PREPROCESSING, TRANSFORMATION, AND LOADING USING TOOLS LIKE PANDAS AND SCIKIT-LEARN

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.datasets import fetch_openml

# Step 1: Extract - Load data
# Example: Load dataset from OpenML
data = fetch_openml(name="titanic", version=1, as_frame=True)
df = data.frame
# Step 2: Separate features and target
X = df.drop("survived", axis=1)
y = df["survived"]
# Step 3: Define column types
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = X.select_dtypes(include=['object', 'category']).columns
# Step 4: Create Transformers
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])
# Step 5: Combine Transformers using ColumnTransformer
preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numerical_cols),
    ('cat', categorical_transformer, categorical_cols)
])
# Step 6: Create the final pipeline
full_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor)
])
# Step 7: Fit and transform the data
X_processed = full_pipeline.fit_transform(X)
# Step 8: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)
# Step 9: Output
print("Processed training data shape:", X_train.shape)
print(" Processed test data shape:", X_test.shape)
print("\n Sample of processed training data:\n")
print(pd.DataFrame(X_train).head())
Processed training data shape: (1047, 2829)
 Processed test data shape: (262, 2829)
Output

 Sample of processed training data:

                                                   0
0  <Compressed Sparse Row sparse matrix of dtype ...
1  <Compressed Sparse Row sparse matrix of dtype ...
2  <Compressed Sparse Row sparse matrix of dtype ...
3  <Compressed Sparse Row sparse matrix of dtype ...
4  <Compressed Sparse Row sparse matrix of dtype ...

## THE ETL PROCESS
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.datasets import fetch_openml
import os
       
def load_data():
    print(" Loading data from OpenML...")
    data = fetch_openml(name="titanic", version=1, as_frame=True)
    df = data.frame
    return df
def preprocess_data(df):
    print(" Preprocessing data...")

    # Separate target
    X = df.drop("survived", axis=1)
    y = df["survived"]

    # Identify columns
    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()

    # Transformers
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])

    # ColumnTransformer
    preprocessor = ColumnTransformer(transformers=[
        ('num', numeric_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

    # Pipeline
    full_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor)
    ])

    X_processed = full_pipeline.fit_transform(X)

    return X_processed, y

def save_data(X_train, X_test, y_train, y_test, output_dir="processed_data"):
    print(f" Saving data to '{output_dir}/'...")
    os.makedirs(output_dir, exist_ok=True)

    pd.DataFrame(X_train).to_csv(os.path.join(output_dir, "X_train.csv"), index=False)
    pd.DataFrame(X_test).to_csv(os.path.join(output_dir, "X_test.csv"), index=False)
    y_train.to_csv(os.path.join(output_dir, "y_train.csv"), index=False)
    y_test.to_csv(os.path.join(output_dir, "y_test.csv"), index=False)
    print(" Data saved successfully!")

def main():
    df = load_data()
    X_processed, y = preprocess_data(df)

    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y, test_size=0.2, random_state=42
    )

    print(" Shapes:")
    print("  X_train:", X_train.shape)
    print("  X_test:", X_test.shape)
    print("  y_train:", y_train.shape)
    print("  y_test:", y_test.shape)

    print("\n Sample of processed data:")
    print(pd.DataFrame(X_train).head())

    save_data(X_train, X_test, y_train, y_test)

if __name__ == "__main__":
    main()
 Loading data from OpenML...
 Preprocessing data...
 Shapes:
  X_train: (1047, 2829)
  X_test: (262, 2829)
  y_train: (1047,)
  y_test: (262,)
  
Output
    
 Sample of processed data:
       0             1         2         3         4         5     6     7     \
0  0.841916 -1.000222e+00 -0.479087 -0.445000 -0.491108  0.000000   0.0   0.0   
1 -0.352091  4.751308e-01 -0.479087 -0.445000 -0.440755  0.000000   0.0   0.0   
2 -1.546098 -9.225717e-01 -0.479087  1.866526  0.896274  0.000000   0.0   0.0   
3 -1.546098  1.329282e+00  0.481288 -0.445000  3.755469 -1.244409   0.0   0.0   
4 -1.546098  2.758687e-16 -0.479087 -0.445000  0.176038  0.000000   0.0   0.0   

   8     9     ...  2819  2820  2821  2822  2823  2824  2825  2826  2827  2828  
0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  
1   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  
2   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  
3   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  
4   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  

[5 rows x 2829 columns]
 Saving data to 'processed_data/'...
 Data saved successfully!
